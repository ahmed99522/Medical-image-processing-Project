{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------- Brain Tumor Segmentation & Classification -------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Images Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_yes = glob.glob(\"C:/Users/moham/Desktop/Medical Image Processing Project/dataset_yes/*.jpg\")\n",
    "path_no = glob.glob(\"C:/Users/moham/Desktop/Medical Image Processing Project/dataset_no/*.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation - Brain With Tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image in path_yes:\n",
    "    input_image = np.array(Image.open(image))\n",
    "    resized = cv2.resize(input_image,(512,512))\n",
    "    if(len(resized.shape) > 2):\n",
    "        \n",
    "        # Convert From RGB to GRAYScale\n",
    "        gray_image = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Thresholding \n",
    "        T, threshold_image = cv2.threshold(gray_image,160,255,cv2.THRESH_BINARY)\n",
    "        \n",
    "        # opening\n",
    "        kernel = np.ones((7,7), 'uint8')\n",
    "        open_img=cv2.morphologyEx(threshold_image,cv2.MORPH_OPEN,kernel)\n",
    "        \n",
    "        # Closing\n",
    "        close_img=cv2.morphologyEx(open_img,cv2.MORPH_CLOSE,kernel)\n",
    "        \n",
    "            \n",
    "        # connected Components in Images\n",
    "        num_labels, labels_im, stats, centroids = cv2.connectedComponentsWithStats(close_img, connectivity = 4)\n",
    "       # print(stats)\n",
    "        for k in range(1,num_labels):\n",
    "                \n",
    "            pts = np.where(labels_im == k)\n",
    "            size = stats[k, cv2.CC_STAT_AREA]\n",
    "            if size > 5000 and size < 50000:\n",
    "                labels_im[pts] = labels_im[pts]\n",
    "            else:\n",
    "                labels_im[pts] =0\n",
    "                    \n",
    "        label_hue = np.uint8(179*labels_im/np.max(labels_im))\n",
    "        # Ignore Dividing By 0 Value Error\n",
    "        np.seterr(divide='ignore' , invalid='ignore')\n",
    "\n",
    "        blank_ch = 255*np.ones_like(label_hue)\n",
    "        labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])\n",
    "        labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)\n",
    "        labeled_img[label_hue==0] = 0\n",
    "         \n",
    "        # Convert to Gray    \n",
    "        Gray = cv2.cvtColor(labeled_img , cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "            \n",
    "        cv2.imwrite('C:/Users/moham/Desktop/Medical Image Processing Project/Result_yes/0'+ str(i)+'.jpg',Gray)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation - Brain With Non-Tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image in path_no:\n",
    "    input_image = np.array(Image.open(image))\n",
    "    resized = cv2.resize(input_image,(512,512))\n",
    "    if(len(resized.shape) > 2):\n",
    "        \n",
    "        # Convert From RGB to GRAYScale\n",
    "        gray_image = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        \n",
    "        # Thresholding \n",
    "        T, threshold_image = cv2.threshold(gray_image,160,255,cv2.THRESH_BINARY)\n",
    "        \n",
    "        # opening\n",
    "        kernel = np.ones((7,7), 'uint8')\n",
    "        open_img=cv2.morphologyEx(threshold_image,cv2.MORPH_OPEN,kernel)\n",
    "        \n",
    "        # Closing\n",
    "        close_img=cv2.morphologyEx(open_img,cv2.MORPH_CLOSE,kernel)\n",
    "        \n",
    "            \n",
    "        # connected Components in Images\n",
    "        num_labels, labels_im, stats, centroids = cv2.connectedComponentsWithStats(close_img, connectivity = 4)\n",
    "        for k in range(1,num_labels):\n",
    "                \n",
    "            pts = np.where(labels_im == k)\n",
    "            size = stats[k, cv2.CC_STAT_AREA]\n",
    "            if size > 5000 and size < 50000:\n",
    "                labels_im[pts] = labels_im[pts]\n",
    "            else:\n",
    "                labels_im[pts] =0\n",
    "                    \n",
    "        label_hue = np.uint8(179*labels_im/np.max(labels_im))\n",
    "        # Ignore Dividing By 0 Value Error\n",
    "        np.seterr(divide='ignore' , invalid='ignore')\n",
    "\n",
    "        blank_ch = 255*np.ones_like(label_hue)\n",
    "        labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])\n",
    "        labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)\n",
    "        labeled_img[label_hue==0] = 0\n",
    "         \n",
    "        # Convert to Gray    \n",
    "        Gray = cv2.cvtColor(labeled_img , cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "            \n",
    "        cv2.imwrite('C:/Users/moham/Desktop/Medical Image Processing Project/Result_no/0'+ str(i)+'.jpg',Gray)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification - Based of Number of Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Image Path: \n",
      "C:\\Users\\moham\\Desktop\\Medical Image Processing Project\\dataset_yes\\Y1.jpg\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Enter Image Path: \")\n",
    "Read_img = input()\n",
    "input_image = np.array(Image.open(Read_img))\n",
    "resized = cv2.resize(input_image,(512,512))\n",
    "if(len(resized.shape) > 2):\n",
    "    # Convert From RGB to GRAYScale\n",
    "    gray_image = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        \n",
    "        # Thresholding \n",
    "    T, threshold_image = cv2.threshold(gray_image,160,255,cv2.THRESH_BINARY)\n",
    "        \n",
    "        # opening\n",
    "    kernel = np.ones((7,7), 'uint8')\n",
    "    open_img=cv2.morphologyEx(threshold_image,cv2.MORPH_OPEN,kernel)\n",
    "        \n",
    "        # Closing\n",
    "    close_img=cv2.morphologyEx(open_img,cv2.MORPH_CLOSE,kernel)\n",
    "        \n",
    "            \n",
    "        # connected Components in Images\n",
    "    num_labels, labels_im, stats, centroids = cv2.connectedComponentsWithStats(close_img, connectivity = 4)\n",
    "        \n",
    "    for k in range(1,num_labels):\n",
    "                \n",
    "        pts = np.where(labels_im == k)\n",
    "        size = stats[k, cv2.CC_STAT_AREA]\n",
    "        if size > 5000 and size < 50000:\n",
    "            labels_im[pts] = labels_im[pts]\n",
    "        else:\n",
    "            labels_im[pts] =0\n",
    "                    \n",
    "    label_hue = np.uint8(179*labels_im/np.max(labels_im))\n",
    "        # Ignore Dividing By 0 Value Error\n",
    "    np.seterr(divide='ignore' , invalid='ignore')\n",
    "\n",
    "    blank_ch = 255*np.ones_like(label_hue)\n",
    "    labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])\n",
    "    labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)\n",
    "    labeled_img[label_hue==0] = 0\n",
    "         \n",
    "        # Convert to Gray    \n",
    "    Gray = cv2.cvtColor(labeled_img , cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    if(num_labels > 1 ):\n",
    "        plt.title('Tumor Founded !')\n",
    "        plt.gray()\n",
    "        plt.axis('off')\n",
    "        plt.imshow(Gray)\n",
    "    else:\n",
    "        plt.title('No Tumor Founded !')\n",
    "        plt.gray()\n",
    "        plt.axis('off')\n",
    "        plt.imshow(Gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "#### TP (True Positive): Existing tumor and detected correctly. |   TN (True Negative): Non-existing tumor and not detected. \n",
    "#### FP (False Positive): Non-existing tumor and detected.          |   FN (False Negative): Existing tumor and not detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.03370786516854"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(51 + 79) /(51+79+38+10) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity\n",
    "#### measure of successful determination of the person having a tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.60655737704919"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "51 / (51 + 10) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specificity\n",
    "#### measure of successful determination of the person not having a tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.52136752136752"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "79 / (79 + 38) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
